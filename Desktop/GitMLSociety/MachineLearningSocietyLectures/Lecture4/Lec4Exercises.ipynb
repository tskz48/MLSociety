{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n",
      "[[0.33905979 0.25084926 0.43632908 0.92057604]\n",
      " [0.29246464 0.40200298 0.42416697 0.32787065]\n",
      " [0.3300373  0.72324688 0.59726866 0.32601114]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "#Question 1\n",
    "\n",
    "inputs = [[1.0, 2.0, 3.0, 2.5], #each row of inputs corresponds to an input sample and each column corresponds to a feature. (There are four features, so 4 input neurones)\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "weights = [[0.2, 0.8, -0.5, 1.0], # the ith row of the weights matrix corresponds to the weights of the ith neuron in the first hidden layer.\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2.0, 3.0, 0.5] #biases for the first hidden layer\n",
    "\n",
    "layer_outputs1 = np.dot(inputs, np.array(weights).T) + biases #calculating the outputs for the neurones in the first hidden layer.\n",
    "print(layer_outputs1)         \n",
    "weights2 = np.random.rand(len(weights), len(weights))# generating a random set of weights for the neurones in the output layer (after the first hidden layer in this case).\n",
    "print(weights2)\n",
    "\n",
    "final_output= np.dot(layer_outputs1, np.array(weights2).T) + biases\n",
    "\n",
    "\n",
    "print(\"Final Output:\", final_output)\n",
    "\n",
    "nputs = [[1.0, 2.0, 3.0, 2.5], #each row of inputs corresponds to an input sample and each column corresponds to a feature. (There are four features, so 4 input neurones)\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "weights = [[0.2, 0.8, -0.5, 1.0], # the ith row of the weights matrix corresponds to the weights of the ith neuron in the first hidden layer.\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2.0, 3.0, 0.5] #biases for the first hidden layer\n",
    "biases2=[0.0] #bias for the output layer\n",
    "\n",
    "\n",
    "layer_outputs1 = np.dot(inputs, np.array(weights).T) + biases #calculating the outputs for the neurones in the first hidden layer.\n",
    "print(layer_outputs1)         \n",
    "weights2 = np.random.rand(1, len(weights))# generating a random set of weights for the neuron in the output layer (after the first hidden layer in this case).\n",
    "print(weights2)\n",
    "\n",
    "final_output= np.dot(layer_outputs1, np.array(weights2).T) +biases2\n",
    "\n",
    "#Question 2\n",
    "\n",
    "\n",
    "def relu(x): #the relu activation function which enables us to account for non-linearity, to model more complex patterns in our samples\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "layer_outputs1=relu(np.dot(inputs, np.array(weights).T) + biases)\n",
    "final_output=relu(np.dot(layer_outputs1, np.array(weights2).T) +biases2)\n",
    "\n",
    "\n",
    "#the relu activation functions sets all the negative values to 0, which introduces sparsity. Therefore, generalisation capabilities are enhanced. Furthermore, sparsity makes the model more efficient, as it avoids redundant computatons. Relu introduces a non symmetric distribution, allowing neurones to learn diverse features. Since relus gradient is 1 for positive values, it avoids the vanish gradient problem that can arise from other activation functions like tanh or sigmoid."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VSCode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
